quiz 1

a1 sum(x * w)/sum(w)

a2 coef(lm(y ~ x - 1)), sum(y * x)/sum(x^2)

a3 
data(mtcars)
summary(lm(mpg ~ wt, data = mtcars))

attach(mtcars)
cor(mpg, wt) * sd(mpg)/sd(wt)
detach(mtcars)

a4
corxy * sdy/sdx

a5
This is the classic regression to the mean problem. We are expecting the score to get multiplied by 0.4. So 1.5*0.4

a6 
((x - mean(x))/sd(x))[1]

a7
coef(lm(y ~ x))[1]


a8
The intercept estimate is $\bar Y - \beta_1 \bar X$ and so will be zero.

a9
mean(x)

a10



quiz 2

1.summary(lm(y ~ x))$coef

2.summary(lm(y ~ x))$sigma

3.data(mtcars)
fit <- lm(mpg ~ I(wt - mean(wt)), data = mtcars)
confint(fit)

4.This is the standard interpretation of a regression coefficient. The expected change in the response per unit change in the predictor.

5.fit <- lm(mpg ~ wt, data = mtcars)
predict(fit, newdata = data.frame(wt = 3), interval = "prediction")


6.fit <- lm(mpg ~ wt, data = mtcars)
confint(fit)[2, ] * 2
##   2.5 %  97.5 % 
## -12.973  -8.405

## Or equivalently change the units
fit <- lm(mpg ~ I(wt * 0.5), data = mtcars)
confint(fit)[2, ]

7.It would get multiplied by 100.

8.This is exactly covered in the notes. But note that if Y=β0+β1X+ϵ then Y=β0−cβ1+β1(X+c)+ϵ so that the answer is that the intercept gets subtracted by cβ1


9.fit1 <- lm(mpg ~ wt, data = mtcars)
fit2 <- lm(mpg ~ 1, data = mtcars)
1 - summary(fit1)$r.squared

sse1 <- sum((predict(fit1) - mtcars$mpg)^2)
sse2 <- sum((predict(fit2) - mtcars$mpg)^2)
sse1/sse2

10.They do provided an intercept is included. If not they most likely won't.



quiz 3

1. fit <- lm(mpg ~ factor(cyl) + wt, data = mtcars)
summary(fit)$coef

2.
fit1 <- lm(mpg ~ factor(cyl), data = mtcars)
fit2 <- lm(mpg ~ factor(cyl) + wt, data = mtcars)
summary(fit1)$coef
##              Estimate Std. Error t value  Pr(>|t|)
## (Intercept)    26.664     0.9718  27.437 2.688e-22
## factor(cyl)6   -6.921     1.5583  -4.441 1.195e-04
## factor(cyl)8  -11.564     1.2986  -8.905 8.568e-10
summary(fit2)$coef
##              Estimate Std. Error t value  Pr(>|t|)
## (Intercept)    33.991     1.8878  18.006 6.257e-17
## factor(cyl)6   -4.256     1.3861  -3.070 4.718e-03
## factor(cyl)8   -6.071     1.6523  -3.674 9.992e-04
## wt             -3.206     0.7539  -4.252 2.130e-04
c(summary(fit1)$coef[3, 1], summary(fit2)$coef[3, 1])
## [1] -11.564  -6.071

3.
fit1 <- lm(mpg ~ factor(cyl) + wt, data = mtcars)
fit2 <- lm(mpg ~ factor(cyl) * wt, data = mtcars)
summary(fit1)$coef
##              Estimate Std. Error t value  Pr(>|t|)
## (Intercept)    33.991     1.8878  18.006 6.257e-17
## factor(cyl)6   -4.256     1.3861  -3.070 4.718e-03
## factor(cyl)8   -6.071     1.6523  -3.674 9.992e-04
## wt             -3.206     0.7539  -4.252 2.130e-04
summary(fit2)$coef
##                 Estimate Std. Error t value  Pr(>|t|)
## (Intercept)       39.571      3.194 12.3895 2.058e-12
## factor(cyl)6     -11.162      9.355 -1.1932 2.436e-01
## factor(cyl)8     -15.703      4.839 -3.2448 3.223e-03
## wt                -5.647      1.359 -4.1538 3.128e-04
## factor(cyl)6:wt    2.867      3.117  0.9197 3.662e-01
## factor(cyl)8:wt    3.455      1.627  2.1229 4.344e-02
anova(fit1, fit2)
## Analysis of Variance Table
## 
## Model 1: mpg ~ factor(cyl) + wt
## Model 2: mpg ~ factor(cyl) * wt
##   Res.Df RSS Df Sum of Sq    F Pr(>F)
## 1     28 183                         
## 2     26 156  2      27.2 2.27   0.12


4.

5.
influence(lm(y ~ x))$hat
##      1      2      3      4      5 
## 0.2287 0.2438 0.2525 0.2804 0.9946
## showing how it's actually calculated
xm <- cbind(1, x)
diag(xm %*% solve(t(xm) %*% xm) %*% t(xm))


6. influence.measures(lm(y ~ x))

7. See lecture 02_03 for various examples.


quiz 4 


1.

library(MASS)
data(shuttle)
## Make our own variables just for illustration
shuttle$auto <- 1 * (shuttle$use == "auto")
shuttle$headwind <- 1 * (shuttle$wind == "head")
fit <- glm(auto ~ headwind, data = shuttle, family = binomial)
exp(coef(fit))
## (Intercept)    headwind 
##      1.3273      0.9687
## Another way without redifing variables
fit <- glm(relevel(use, "noauto") ~ relevel(wind, "tail"), data = shuttle, family = binomial)
exp(coef(fit))

2.
shuttle$auto <- 1 * (shuttle$use == "auto")
shuttle$headwind <- 1 * (shuttle$wind == "head")
fit <- glm(auto ~ headwind + magn, data = shuttle, family = binomial)
exp(coef(fit))
## (Intercept)    headwind  magnMedium     magnOut  magnStrong 
##      1.4852      0.9685      1.0000      0.6842      0.9376
## Another way without redifing variables
fit <- glm(relevel(use, "noauto") ~ relevel(wind, "tail") + magn, data = shuttle, 
    family = binomial)
exp(coef(fit))

3. Remember that the coefficients are on the log scale. So changing the sign changes the numerator and denominator for the exponent.

4. fit <- glm(count ~ relevel(spray, "B"), data = InsectSprays, family = poisson)
exp(coef(fit))[2]
## relevel(spray, "B")A 
##               0.9457

5. Question Explanation

Note, the coefficients are unchanged, except the intercept, which is shifted by log(10). Recall that, except the intercept, all of the coefficients are interpretted as log relative rates when holding the other variables or offset constant. Thus, a unit change in the offset would cancel out. This is not true of the intercept, which is interperted as the log rate (not relative rate) with all of the covariates set to 0.

6. z <- (x > 0) * x
fit <- lm(y ~ x + z)
sum(coef(fit)[2:3])